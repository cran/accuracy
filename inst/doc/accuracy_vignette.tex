%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% LATEX HEADERS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[11pt]{article}
\usepackage{rotating}
\usepackage[dvips]{epsfig}
\usepackage{multirow}
\usepackage{amsfonts,amssymb,amsmath}
\usepackage{array}
\usepackage{url}
\usepackage{thumbpdf}

\usepackage{vmargin}
    \setpapersize{USletter}
    \setmarginsrb{1.0in}{1.0in}{1.0in}{1.0in}{0.5in}{0.2in}{0in}{0.2in}
\renewcommand{\baselinestretch}{1.25}
\renewcommand{\bibitem}{\vskip 2pt\par\hangindent\parindent\hskip-\parindent}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% SHORTCUTS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newcommand{\y}{\mathbf{y}}
\newcommand{\Y}{\mathbf{Y}}
\newcommand{\x}{\mathbf{x}}
\newcommand{\X}{\mathbf{X}}
\newcommand{\B}{\boldsymbol{\beta}}
\newcommand{\cov}{\text{Cov}}
\newcommand{\var}{\text{Var}}
\newcommand{\T}{\boldsymbol{\theta}}
\newcommand{\EP}{\boldsymbol{\epsilon}}

%%%%%%%
%%% Vignette Headers
%%%%%%%

%\VignetteKeywords{statistical computing}
%\VignetteVersion{1.0}
%\VignetteTitle{Tools For Accurate and Reliable Statistical Computing} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% DOCUMENT STARTS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{/nfs/fs1/home/M/maltman/R-rc/share/texmf/Sweave}
\begin{document}

%% Automatically supplied by Jstatsoft headers
%
%\begin{large}\begin{center}
%    \textsc{``R Modules for Accurate and Reliable Statistical Computing''}\\[22pt]
%\end{center}\end{large}
%\vspace{11pt}
%\begin{center}
%    Micah Altman, \texttt{Micah\_Altman@harvard.edu}\\
%    Jeff Gill, \texttt{jgill@ucdavis.edu}\\
%    Michael McDonald, \texttt{mmcdon@gmu.edu}\\
%\end{center}
%


\newpage

\section{Using data perturbations for sensitivity analysis}

An easy-to-use exploratory test for numerical and measurement error stability for a given model is 
to introduce small random perturbations to the data, on the order of the measurement error of the 
instruments used to collect it, and recalculate the estimate. When the estimates produced using 
this technique vary greatly, the model estimation is necessarily unstable. And although the 
converse is not necessarily true, where a model is already known to be statistically appropriate,
this type of sensitivity analysis will give the researcher greater confidence that the their 
results are robust to numerical and measurement error.

We have developed a package in R that makes perturbation-based sensitivity
 analysis simple to apply and to interpret. For most models this running
 a sensitivity analysis involves only two steps.                             
\begin{enumerate}
\item Specify the data, model, and model options for the unperturbed model, and optionally,
the error functions for the perturbation.  
\item Use \texttt{summary()} or \texttt{plot(summary())} to see the sensitivity of the parameter estimates
to perturbations.
\end{enumerate}
Perturb works automatically almost with any \textbf{R} model, such as \texttt{lm}, 
\texttt{glm}, and \texttt{nls}, that accepts \texttt{data} 
as an argument to supply data and  that returns estimated coefficients through
\texttt{coef()}.

The example below shows how to conduct a sensitivity analysis of the classic 
analysis by Longley (1964) using \texttt{sensitivity()} and default noise
functions. 

\vbox{
\begin{Schunk}
\begin{Sinput}
> plongley = sensitivity(longley, lm, Employed ~ .)
> print(summary(plongley), digits = 4)
\end{Sinput}
\begin{Soutput}
[1] "Sensitivity of coefficients over  50  perturbations:"
             Mean Perturbed beta (Original Beta) (Original Stderr)       2.5%
(Intercept)           -3.161e+03      -3.482e+03         8.904e+02 -4.475e+03
GNP.deflator           1.980e-02       1.506e-02         8.491e-02 -1.408e-01
GNP                   -3.085e-02      -3.582e-02         3.349e-02 -8.325e-02
Unemployed            -1.932e-02      -2.020e-02         4.884e-03 -2.776e-02
Armed.Forces          -9.838e-03      -1.033e-02         2.143e-03 -1.329e-02
Population            -2.500e-02      -5.110e-02         2.261e-01 -3.280e-01
Year                   1.662e+00       1.829e+00         4.555e-01  5.450e-01
                  97.5% [Out of Bounds]
(Intercept)  -9.775e+02               *
GNP.deflator  1.566e-01                
GNP           3.804e-02               *
Unemployed   -8.603e-03               *
Armed.Forces -6.549e-03                
Population    3.009e-01                
Year          2.327e+00               *
\end{Soutput}
\end{Schunk}
}
\vbox{
The sensitivity results can also be expressed in plot format:
\begin{Schunk}
\begin{Sinput}
> plot(summary(plongley))
\end{Sinput}
\end{Schunk}
\includegraphics{accuracy_vignette-003}
}

This is a rare example of a model that is very sensitive to noise.
Even so, note that the small amounts of  noise applied 
tremendously alter some of the estimated coefficients, but not others.  In most 
practical cases, however,  the substantive implications of your model will remain the same
across the sensitivity analysis -- in which case, you can publish them with
greater confidence. 

If error functions are not specified, a default  set of error function will be 
selected based on measurement types of the variable: continuous, ordered, or unordered.
Continuous variables, by default are subject to a small amount of
 mean-zero component-wise uniformly distributed
noise, which is typical of instrumentation-driven measurement error. Ordered factors
are assigned a small probability of having observations reclassified to the neighboring classification,
and unordered factors have a small probability of being reassigned to another legal value.

Alternatively, one can specify the error functions to use yourself, or use one of many
supplied by \emph{accuracy}.  The \emph{accuracy} package comes with a wide range 
of noise functions for continuous distributions,  and random reclassification of factors. 
\footnote{The \texttt{perturb} package for collinearity diagnosis by Hendrickx, et. al (2004) (which was
developed for \texttt{R} after the \texttt{accuracy} package) provides additional
methods for randomly reclassifying factors that via its \texttt{reclassify()} function. 
This function can be used in conjunction with \texttt{accuracy}. 
Hendrickx, et. al also provide a number of collinearity 
diagnostics, including one based on data perturbations. } 

Your choice of error functions should be chosen to reflect measurement error model for the specific
data you are using. In numerical analysis,  uniform noise  is often used since this is what would be
expected from  simple rounding error.  Normal random noise is commonly used in statistics,
under the assumption that measurement error is the sum of multiple independent error processes. In 
addition, when normal perturbations are used, the result can be interpreted, for many models, as equivalent
to the results of running a slightly perturbed \emph{model} on unperturbed data.  In some cases,
like discrete or ratio variables, other forms of noise are necessary to preserve
the structure of the problem. (see for example, Altman, Gill, McDonald 2005). 
The magnitude of the noise is also under the control of the researcher. Most
use a magnitude equivalent to the researchers estimate of the underlying measurement error in the data.
Noise is usually adjusted to the size of each component, since this better
preserves the structure of the problem, however in some cases the underlying
measurement error model may imply norm-wise scaling of the noise. For more
information on noise distributions and measurement error models see , e.g., 
Belsley 1991, Chaitin-Chatelin \& Traviesas-Caasan  (2004b), Caroll et. al (1995), Cheng \& Van Ness (1999), Fuller (1987).

If multiple plausible measurement error models can be hypothesized,
we recommend that \texttt{sensitivity} be run multiple times with different noise specifications,
However, in our  experience with  social science analyses, the choice
of error model does not tend to  effect, in practice, the substantive conclusions from the 
sensitivity analysis.

Some researchers omit perturbations to outcome variables, since, in terms of
statistical theory, mean-zero measurement error on outcome variables (as opposed to explanatory variables)
contribute only to increased variance in estimates, not bias. While this attitude is well-justified 
in the context of statistical theory, it is not similarly justified in the computational realm. If the estimation
of a model is computationally unstable, errors in the outcome variable may have large and unpredictable
 biases on the model estimate. Hence, the conservative default in our package is to subject all variables to perturbation, although options are available to completely control the form and magnitude of
all perturbations.

\vbox{
Consider this example, which shows a sensitivity analysis of the anorexia analysis
described in Venables and Ripley (2002). In this case, we leave the dependent
variable unperturbed, by assigning it the \emph{identity} error function.

\begin{Schunk}
\begin{Sinput}
> data(anorexia, package = "MASS")
> panorexia = sensitivity(anorexia, glm, Postwt ~ Prewt + Treat + 
+     offset(Prewt), family = gaussian, ptb.R = 100, ptb.ran.gen = c(PTBi, 
+     PTBus, PTBus), ptb.s = c(1, 0.005, 0.005))
> print(summary(panorexia), digits = 4)
\end{Sinput}
\begin{Soutput}
[1] "Sensitivity of coefficients over  100  perturbations:"
            Mean Perturbed beta (Original Beta) (Original Stderr)    2.5%
(Intercept)             49.7361         49.7711           13.3910 49.0704
Prewt                   -0.5651         -0.5655            0.1612 -0.5736
TreatCont               -4.0943         -4.0971            1.8935 -4.1683
TreatFT                  4.5608          4.5631            2.1333  4.4788
              97.5% [Out of Bounds]
(Intercept) 50.4577                
Prewt       -0.5571                
TreatCont   -4.0227                
TreatFT      4.6457                
\end{Soutput}
\end{Schunk}
}

Finally, if a model in \texttt{R}  does not take a \texttt{data} argument or does not
return coefficients through the  \texttt{coef} method, it is usually 
only a matter of a few minutes to write a
small wrapper that calls the original model with appropriate data, and that
provides a  \texttt{coef} method for retrieving the results. (Alternatively, you
might to choose to run such models in \texttt{Zelig}, as described in the next section.)

For example, the \texttt{mle} function for maximum-likelihood estimation does
not have an explicit \texttt{data} option. Instead, it normally receives data implicitly
through the log-likelihood function, \texttt{ll}, passed into it. To adapt it for use
in \texttt{sensitivity} we simply construct a another function that accepts data and a 
log-likelihood function separately, constructs a temporary log-likelihood
function with the data passed in the environment, and then calls \texttt{mle}
with the temporary function:
\vbox{
\begin{Schunk}
\begin{Sinput}
> mleD <- function(data, lld, ...) {
+     f = formals(lld)
+     f[1] = NULL
+     ll <- function() {
+         cl = as.list(match.call())
+         cl[1] = NULL
+         cl$data = as.name("data")
+         do.call(lld, cl)
+     }
+     formals(ll) = f
+     mle(ll, ...)
+ }
\end{Sinput}
\end{Schunk}
}

Finally, construct the log-likelihood function to accept data. As in this example, which
is based on the documented example in the \texttt{Stats4} package:

\vbox{
\begin{Schunk}
\begin{Sinput}
> library(stats4)
> dat = as.data.frame(cbind(0:10, c(26, 17, 13, 12, 20, 5, 9, 8, 
+     5, 4, 8)))
> llD <- function(data, ymax = 15, xhalf = 6) -sum(stats::dpois(data[[2]], 
+     lambda = ymax/(1 + data[[1]]/xhalf), log = TRUE))
> print(summary(sensitivity(dat, mleD, llD)), digits = 4)
\end{Sinput}
\begin{Soutput}
[1] "Sensitivity of coefficients over  50  perturbations:"
      Mean Perturbed beta (Original Beta)    min   2.5%  97.5%    max
ymax               25.057          24.993 23.311 24.566 25.192 29.989
xhalf               3.053           3.057  2.355  2.908  3.339  3.391
\end{Soutput}
\end{Schunk}
}

\subsection[Sensitivity analysis using Zelig]{Sensitivity analysis using \texttt{Zelig}}

\texttt{Zelig} (Imai, et. al 2005) is an easy-to-use R package that can estimate and help interpret
 the results of a large range of statistical models. \texttt{Zelig} provides
 a uniform interface to these models the \texttt{Accuracy} package
utilizes to enable sensitivity analyses. In addition, \texttt{Accuracy} can
also be used to perform sensitivity analyses of the robust alternatives, simulated 
predicted values, expected values, first differences, and risk ratios that \texttt{Zelig}
produces for all the models it supports.  \footnote{ \texttt{Zelig} also
 integrates nonparametric matching methods as an optional preprocessing step. Thus
 \texttt{Accuracy} supports sensitivity analysis of models subject to such pre-processing as well.}
So, using these packages together is an easy way to analyze the sensitivity 
of \emph{predicted values} to measurememnt error.


To illustrate, we replicate Longley's analysis (above), using
\texttt{zelig()} (instead of \texttt{lm()}) to run the OLS model, and the convenience function
 \texttt{sensitivityZelig()} to run the sensitivity analysis:
\vbox{
\begin{Schunk}
\begin{Sinput}
> if (require("Zelig", quietly = T, warn.conflicts = F)) {
+     zelig.out = zelig(Employed ~ GNP.deflator + GNP + Unemployed + 
+         Armed.Forces + Population + Year, "ls", longley)
+     perturb.zelig.out = sensitivityZelig(zelig.out)
+ }
\end{Sinput}
\begin{Soutput}
## 
##  Zelig (Version 2.6-5, built: 2006-09-14)
##  Please refer to http://gking.harvard.edu/zelig for full documentation 
##  or help.zelig() for help with commands and models supported by Zelig.
##
\end{Soutput}
\end{Schunk}
}

Just as above, \texttt{summary()} and \texttt{plot(summary())} can be used summarize
the sensitivity of the model coefficients.  In addition, we can use the 
\texttt{Zelig} methods \texttt{setx} and \texttt{sim} to simulate various
quantities of interest. And when \texttt{summary()} and
 \texttt{plot()} are used, they will display a \emph{sensitivity analysis} 
of the predicted values.

For example, this code generates predictions of the distribution of the explanatory variable,
`Employed', around the  point where `Year' equals 1955, 
and the other variables are at their means, and creates a profile plot of 
the predicted distribution of the explanatory variable:

\vbox{
\begin{Schunk}
\begin{Sinput}
> if (require("Zelig", quietly = T, warn.conflicts = F)) {
+     setx.out = setx(perturb.zelig.out, Year = 1955)
+     sim.perturb.zelig.out = psim(perturb.zelig.out, setx.out)
+     print(summary(sim.perturb.zelig.out))
+ }
\end{Sinput}
\begin{Soutput}
**** 50  COMBINED perturbation simulations 

  Model: ls 
  Number of simulations: 1000 

Values of X 
     (Intercept) GNP.deflator   GNP Unemployed Armed.Forces Population Year
1947           1        101.7 387.7      319.3        260.7      117.4 1954

Expected Values: E(Y|X)
      mean     sd 2.5% 97.5%
1947 65.31 0.1066 65.1 65.52
\end{Soutput}
\end{Schunk}
\begin{Schunk}
\begin{Sinput}
> if (require("Zelig", quietly = T, warn.conflicts = F)) {
+     plot(sim.perturb.zelig.out)
+ }
\end{Sinput}
\begin{Soutput}
**** 50  COMBINED perturbation simulations 
\end{Soutput}
\end{Schunk}
\includegraphics{accuracy_vignette-009}
}


\subsection{True random numbers through entropy collection}

`Random' numbers aren't.  The numbers provided by routines such as \texttt{runif()} are not genuinely random.
 Instead, they are \emph{pseudo-random number generators}
(PRNGs), deterministic processes that create a sequence of numbers. 
Pseudo-random number generators start with a single ``seed'' value (specified by the user or left at defaults)
 and generate a repeating sequence with a certain
fixed length, or  period $p$. This sequence is statistically
similar, in limited respects, to random draws from a uniform
distribution.

The earliest PRNGs, still in use in some places, and used in early versions of R, is the Linear Congruential Generator (LCG), 
which is defined as:

\begin{align}\label{Congruential.Generator}
    LCG(a,m,s,c)&\equiv \nonumber\\
            x_{0} &=s,  \nonumber\\
            x_{n} &=(ax_{n-1}+c)\bmod{m}.
\end{align}
(All parameters are integers, and in practice $x$ is usually divided by $m$ to yield numbers between zero and one.) 

This function  generates a sequence of numbers between $[0,m-1]$ which appears to be, using some tests, uniformly distributed in that range.  Other PNRG's are more complex, but share with the LCG the  fundamental properties of determinism and periodicity.  See (Gentle 1998) for an extensive treatment of modern PRNG's and theory.

\texttt{R} provides several high quality PRNG's natively, and packages such as  
\texttt{gsl}, \texttt{rstream} and \texttt{rpsrng} which  can be used to generate
quasi-random number streams, and concurrent  PRNG streams.
Regardless of the particular PRNG algorithm used, however, a PRNG cannot
perfectly mimic a random sequence. And, in fact, there is no complete
theory to describe the domains for which PRNG and true random sequences 
can be considered interchangeable.  In addition, the theory
on which PRNG's are based assumes that the seed itself is \emph{truly} random.

The \texttt{runifT()} routine is different from other random number generators in R. 
It delivers true random numbers based on
entropy collected from external physical sources of randomness.

Two sources of randomness are currently supported. On Unix and Linux system, the kernel gathers environmental noise from 
device drivers and other sources into a system entropy pool.
This pool can be accessed through the '/dev/random' pseudo-device.
Alternatively, the ``Hotbits'' web server, run by FourmiLab provides random bytes based on radioactive decay.

Using either source, these routines will retrieve random bits in chunks,
 and keep them in a local pool. This pool will be used as necessary to 
 satisfy calls to \texttt{runifT()} and \texttt{resetSeed()},
  and will be automatically refreshed from the external sources when empty. 
  If external sources are unavailable, the pool is refreshed using standard PRNG's.

Entropy collection is relatively slow compared to PRNGS. So, these routines
are most efficient for generating either small numbers of very-high-quality random
numbers (e.g. for cryptography) or for seeding (and regularly reseeding) PRNG's. 
The function \texttt{resetSeed()} sets the seed for the standard PRNG\'s using
true random bits.  The \texttt{runifS()} automates this process further, by  reseeding 
runif() with random values, periodically to improve the random properties of the
 resulting sequence:

\vbox{
\begin{Schunk}
\begin{Sinput}
> birthday <- function(x, n = 2^20) {
+     spacings = diff(trunc((x * .Machine$integer.max)%%n))
+     tab = table(spacings)
+     tab = tab[which(tab > 1)]
+     chisq.test(sample(tab, 200, replace = T))
+ }
> resetSeed()
\end{Sinput}
\begin{Soutput}
  [1]         403         584  1574998105  1219947389 -1566968000  1543238499
  [7]   722591962  -106145169    61533776   269383546 -1076082683  1509917898
 [13]  1929015882     6029814 -1940921092 -1823930354  -284967753   -92294906
 [19]  1753354895   288841967   397707115   418962571 -1377845613   159710425
 [25]    78527353  1526152204  1102030396  -856366293 -1608516536   148224661
 [31]  -142880580 -1940300624 -2035255238 -2088248949  1614781958  1592901478
 [37]  -753384749  -842292477 -1982288853   453315873  -146107097 -1563745977
 [43]   216733805  2125886982 -1613077458   941899663 -1635470548  -591888092
 [49]  1495883126   362769538 -1157753558   -46582993  -350422784  -550460842
 [55] -1948571063  1063286780  -849293658 -1134245693   221496003   215878923
 [61]  1481709318   -81253551  1428957324   441450185   -18174038   177553064
 [67] -1394732657   537544543 -1791815365  -336097914   150741404  -356175613
 [73]  -943219926  1598086236  1142986852   532838288  1191068178  1506303617
 [79] -1281160451 -1113560099 -1605339792 -1801021010    35441161   -85907336
 [85] -1184411372   173327820  -237258153  -126864993  -738696850  -521367444
 [91]  -857926835   715979243  1684022803   -92482450 -1323963066 -1620432166
 [97]  1505404320   905299656  1236667019 -1527142101   -51902649  -900374510
[103] -1351382442  1911120299 -1158877286 -1682933709 -1844987521 -1462300595
[109]  1821042603  1917347437 -1025697776 -1126981563  -816649679  1388896103
[115]  -163797899 -1024311138   714012216  1988735214 -1962202141  -314366228
[121]  1853755933   734550662   468978162  1360638867  1340154156 -1517180018
[127]  -219706084  1232453484 -1014755624 -1921966827    56415100  -513985431
[133]  1429711951   894327137  1284276335  1488097975  2081816316  1054455238
[139]   -60526153  1219430142   951064286   295439926  -882976041  1491808111
[145] -1260911258   839854079  1312219335  -921022918  -632976316  1347867161
[151]   -26200941   -62497004 -2012183187    30690156   229428689   673278853
[157]  1496340151   -91065035 -1440696328 -2142016029   294451281   131842294
[163]  1294366024   920200829  -812719529  -678431019 -1834522914 -1779244587
[169]  1667544415    34887439  -124615868 -1553017181  -656543585 -1767023029
[175]  -389039721 -1143272944 -1086390367 -1881216125  1170453917 -1954458320
[181]   469395155  -313071219  -408809500 -1717211979 -1638963979  -506618189
[187]   708937969   694821581  1559904240  -967943822  1876016358  -453396153
[193] -1453452094 -1448244322  1115132288   446500774 -1417114223  -878114573
[199]  1564115610 -2090646954 -1518092515 -1754083102  1352981785 -2000558384
[205]  -804836443  1037375648  -840112033  1071514302  1019969974  -411499631
[211]  1310435768  -829099562     1369647   683176804 -1277938537 -2131704919
[217]   218668338 -2129598524  -261332033  1423286550   883843242  -560573674
[223] -1537443790  1919031786   787887596  1992923683  1687021416  1641339470
[229]  2020300077   481652473  -647986182   954272720  1098094719  1602131727
[235]   -11203663 -1626644032  1469963266  1437589583 -1301189616 -1324041405
[241]   743491949  2025241669 -1348041030 -1334122073  1649323012   855144119
[247]   349397455   869549872  1597189278  -704395266   996170591  1579906313
[253]  1014665757  -784474883 -1438332677   704969912  -115980188   785819506
[259] -1376976046  2064557789  -202660681 -1345826778  1043707134 -2002597513
[265] -2052355470 -2040230255   937302523   442447841   176113282  -141882582
[271] -1220668886 -1378761472  1731146642   860675695 -1639234448  1325309045
[277] -2056548180   114285281  -530980722 -1752437139 -1546924179   583781366
[283]  1325210967  -815364436 -1523049720  -434011719  2034915034   700055424
[289]  1906450816   156453988   744011064  1713087600    87046245   439449519
[295] -1457114703   353694359  1008475561   362607533 -1041049810   196046977
[301] -1239274060  -957038615  1953221263  1816374785  1853491592   439601560
[307]  1517303407 -1770247601  1601205930  2129018900 -1863033349  1220337058
[313] -1083853138  1747589457 -1486333064 -1422486156  -817284897  1790852174
[319]   516023299  2046832892 -1632213073  1936494990   202567558  2039984561
[325]  1304472216   231431511  -230343737  2055205297  -521357354  -837264035
[331] -1708626407 -1706399513 -1560613199  -966293589  -554707106  1014161498
[337]  1206881458  -936138678   642441744 -1962305449 -1779146567  1718248136
[343]  -866114850 -1959270789  -366618822 -1690339601  1987298150  1968819070
[349]  1696346336  1166053869   360567803   584086144 -1574739221  -602227538
[355] -1006083044  -254250009   478821195 -1920605832    75185146   477595950
[361]  -135232417  -535824824 -2030423416   663549621   661681697  -156469130
[367]  2043500424  -609442819   364512393  -796556411  -494382845  -917395763
[373]  -268119940  1613224292 -1315740947 -1554768189  -870920342  1014879160
[379]  -168380878  2002976922 -1653884540 -1344086679    -5163296 -1284658548
[385]  1034737159 -1372803662  1388421221  1031089763   413354670  1687016832
[391]  1373642655   334253289   451128433 -1559763930 -1093159304  -174370408
[397]   -79588726 -1844003171  1147099951 -1527831315  -775827851 -1100460659
[403]   298794012 -1189246184  -273354932  1361832703  1090952196  1126495003
[409] -1318597789 -1708459097  -857347268  1800742714  -858667967   529624980
[415]   424303345  -328856714   800715756  -153940422  1738953990   639989490
[421] -1722838333  1736018665 -1284760002  1690444088  1526238297 -1944436786
[427] -1877837945  1413863277  1554501133   410319165 -1801476688   -19858931
[433]   130757936  -155563925  -771805463  1692863305  1334253701 -1338437222
[439]  1091458024 -1474217416 -1998916451  -276937648  2021775823  -894325636
[445] -1308055150   192037603  1776399291   703294281  1581528839   726453184
[451]   902868827   653009298   444297253   -38369607  1532363925  1921157194
[457]  -819216280  1785364670 -1041533615   110244714  -346934293 -1544140511
[463]  -552317564  1423044585  1903450578  -496030737  1937102416    75532627
[469]  -285622063  -209779801    31612594  1939831267  -860457475   728570261
[475]  -743941172  -539751055  1794029052  -374886254   -86250806   381151169
[481] -1172030728   -82951077 -1719930205   169412214 -1621206304  -655196551
[487] -1983850125  -633364427   586860094  -463379074    29428606  1280887581
[493]  -837413840  1677145840  2116868299  -932661473  -949702881   784196602
[499]  -756560421   805377636  1053174946  1815874404  2010949854   398251728
[505]   688423037 -1737633372  -302073272  1560320845 -1704584652   178219542
[511]   829463803  1210431435  2070662675   915155015   303720638  1435733734
[517]  -388345592  1353575827  -320419757   161293244 -1910441474  -494992580
[523]    -6494573  -665424473  -947895069  -403265129  1691746335  1762678513
[529]   381031895   312113042  -809885002  1098570418  -714106526   732134234
[535]  -552751720 -1335461905   -18729132 -1062452383 -1035607472  1773249128
[541]  -114679337    32694918 -1819848512 -1638371124  -851405969   338406804
[547]   191517111  -903968101  1818658233  1384522368   242192372  1170974999
[553]  -282049381  -285111593  1680966089  1433575560    62099560 -1979134419
[559] -1813671646 -1099804199  1968376185  -322468730 -2021867174  1696952924
[565]   217416873   -62360199  1315596411  1478555728  1615349461   233828969
[571]  1892278333  1876618409   659522640  1894344462 -1721449677  1448559390
[577]  1596104589  1618879138 -2022368880  -281369941   904674948  2113952466
[583]   582767550   934361097  2040208307 -1473589684  -273486834  1940459748
[589] -1352028329   639791459 -2059932841  1294784440  -825289823 -1287926453
[595]  1108900564  -842835587  2050618938   123950446   317301060  -808016031
[601] -1941654155   514608497  -598863819  1969108099 -1857606683  2045212288
[607]  1827035988  2075783272   778702882   892768188  1177920702 -1695716767
[613] -2069849766  1107114024  -953217546   465453518  -772239468  2012470515
[619]  1116749711  1510543479   927617099    70857435  -964308413  -385081183
[625]   461904750 -1280859172
\end{Soutput}
\begin{Sinput}
> y = runif(1e+06)
> birthday(y)
\end{Sinput}
\begin{Soutput}
	Chi-squared test for given probabilities

data:  sample(tab, 200, replace = T) 
X-squared = 20.75, df = 199, p-value = 1
\end{Soutput}
\begin{Sinput}
> y = runifS(1e+06)
> birthday(y)
\end{Sinput}
\begin{Soutput}
	Chi-squared test for given probabilities

data:  sample(tab, 200, replace = T) 
X-squared = 32.07, df = 199, p-value = 1
\end{Soutput}
\end{Schunk}
}
\subsection{Tests for global optimality}

The estimation of many statistical models rests on finding the global optimum
to a user-specified non-linear function. R provides a number of tools for such
estimations, including \texttt{nlm()}, \texttt{nls()}, \texttt{mle()}, 
\texttt{optim()}  and \texttt{constrOptim()}. 

All of these functions rely on local search algorithms, and the results they
return may depend on the starting point of the search. Maximum likelihood functions, non-linear-regression models, and the like, are not guaranteed to 
be globally convex in general. And even where convexity is guaranteed by statistical theory, inaccuracies in statistical computation can sometimes induce false local optima (discontinuities that may cause local search algorithms to converge, or at least stop).  A poor or unlucky choice of starting values may cause a search algorithm to converge at a local optimum, which may be far from the real global optimum of the function.  Inferences based on the values of the parameter at the local optimum will be incorrect. 

Knowing when a function has reached its true maximum is
something of an art. While the plausibility of the solution
in substantive terms is often used as a check, relying solely on the expected
answer as a diagnostic might bias researchers toward Type I errors. Diagnostic
tests are therefore useful to provide evidence that computed
solution is the true solution.

A number of strategies related to the choice of   starting values
 have been formalized as tests or global optimality. In this package we 
 implement two. The `Starr' test and the `Dehaan' test.
\footnote{In addition to these tests, the \texttt{R} user may also wish to investigate
the \texttt{bhat} package, which can generate diagnostic profile likelihood plots.} 
\footnote{If this indicats that the optimum has not been reached, the user may consider
using  heuristics designed for non-smooth optimization problems, such as the simulated annealing option for \texttt{optim()}, or the optimizers provided by  the \texttt{gafit}, \texttt{genalg}, 
\texttt{rgenoud} modules.}
 
The intuition behind the Starr test statistic is to run the optimization from 
different starting points to observe 'basins of attraction', and then to
estimate the number of \emph{unobserved} basins of attraction from the
number of observed basins of attraction.  The greater the number
of observed basins of attraction, the lower the probability that a
global optimum has been located.  This idea has been attributed to Turing (1948),
and the test statistics was developed  by Starr (1979):
\begin{equation}\label{Starr.test.equation}
    V_{2}=\frac{S}{r}+\frac{2D}{r\left( r-1\right)}.
\end{equation}\index{optimization algorithm!global optimality
tests!Starr test} Here $V_2$ is the probability a convergence
point has not been observed, and $r$ is the number of randomly
chosen starting points. $S$ is the number of convergence points
that were produced from one (or a {\underline{S}}ingle) starting
value and $D$ is the number of convergence points that were
produced from two (or {\underline{D}}ouble) different starting
values. 

Finch, Mendell, and Thode (1989) demonstrate the value of the
statistic by analyzing a one parameter equation on a $[0,1]$
interval for $r = 100$. While the proposed statistic given by the
above equation is compelling, their example is similar to an
exhaustive grid search on the $[0,1]$ interval. 
(Starr's result is further generalizable for triples and
higher order observed clumping of starting values into their
basins of attraction, but Finch, Mendell, and Thode assert that
counting the number of singles and doubles is usually sufficient.)

The statistic may be infeasible to compute for an unbounded parameter space with 
high dimensionality. However, the intuition behind the statistic
can still  be soundly applied in these cases. If multiple local optima are identified over the
course of a search for good starting values, a researcher should
not simply stop once an apparent best fit has been found,
especially if there are a number of local optima which have basins
of attraction that were identified only once or twice. Our implementation
of the Staff test provides a ready-to-use-interface that can be 
easily incorporated into a search of the parameter space for good  optimization 
starting values.

For computationally intensive problems, another test, by Veall (1990), drawing upon a 
result presented by de Haan (1981), may be more practical. The de Haan/Veall test relies on
 sampling the optimization function itself rather than
identifying basins of attraction. A confidence interval for 
the value of the likelihood function's global optimum is generated from
the points sampled from the likelihood surface. This procedure is much faster than the Starr
test because  the likelihood function  is calculated  only once for each 
trial. As with starting value searches, researchers are
advised to increase the bounds of the search area and the number
of trials if the function to be evaluated has a high degree of
dimensionality or a high number of local optimum have been
identified.

Veall suggests that by using a random search and applying extreme asymptotic theory, a confidence interval for the candidate
solution can be formulated. The method, according to Veall (1990:
1460) is to randomly choose a large number, $n$, of values for the
parameter vector using a uniform density over the entire parameter
space. Call the largest value of the evaluated likelihood function
$L_1$ and the second largest value $L_2$. The $1-p$ confidence
interval for the candidate solution, $L^{'}$, is $[L_1,L^p]$
where:

\begin{equation}\label{Veall.test.equation}
    L^p =L_1+\frac{ L_1-L_2 }{ p^{-1/\alpha}-1 }
\end{equation}
and $\alpha = k/2$, where $k$ is some function that depends on $n$ such that
$k(n)\rightarrow 0$, as $k(n),n \rightarrow \infty$ (a likely candidate is
$k=\sqrt{n}$).

As Veall (1990: 1461) notes, the bounds on the search of the
parameter space must be large enough to capture the global maximum
and $n$ must be large enough to apply asymptotic theory. In Monte
Carlo simulations, Veall suggests that 500 trials are sufficient
for rejecting that a local optimum is not the \emph{a priori}
identified global optimum. 

\vbox{
Examples of applying both the dehaan and starr tests are below:
\begin{Schunk}
\begin{Sinput}
> data("BOD")
> stval = expand.grid(A = seq(10, 100, 10), lrc = seq(0.5, 0.8, 
+     0.1))
> llfun <- function(A, lrc) -sum((BOD$demand - A * (1 - exp(-exp(lrc) * 
+     BOD$Time)))^2)
> lls = NULL
> for (i in 1:nrow(stval)) {
+     lls = rbind(lls, llfun(stval[i, 1], stval[i, 2]))
+ }
> fm1 <- nls(demand ~ A * (1 - exp(-exp(lrc) * Time)), data = BOD, 
+     start = c(A = 20, lrc = log(0.35)))
> ss = -sum(resid(fm1)^2)
> dehaan(lls, ss)
\end{Sinput}
\begin{Soutput}
[1] TRUE
\end{Soutput}
\end{Schunk}
}

\vbox{
\begin{Schunk}
\begin{Sinput}
> llb = NULL
> for (i in 1:nrow(stval)) {
+     llb = rbind(llb, coef(nls(demand ~ A * (1 - exp(-exp(lrc) * 
+         Time)), data = BOD, start = c(A = stval[i, 1], lrc = stval[i, 
+         2]))))
+ }
> starr(llb)
\end{Sinput}
\begin{Soutput}
[1] 0
\end{Soutput}
\end{Schunk}
}
       
\subsection{A generalized Cholesky method}

The generalized inverse is a commonly used technique
in statistical analysis, but the generalized Cholesky has not before
been used for statistical purposes, to our knowledge.
When the inverse of the negative Hessian does not exist, we suggest
two separate procedures to choose from.  One is to create a
\emph{pseudo-variance matrix} and use it, in place of the inverse, in
an importance resampling scheme.  In brief, applying a generalized
inverse (when necessary, to avoid singularity) and generalized
Cholesky decomposition (when necessary, to guarantee positive
definiteness) together often produce a pseudo-variance matrix for the
mode that is a reasonable summary of the curvature of the posterior
distribution. This method is developed and analyzed in detail in (Gill and King, 2004),
here we provide a brief sketch.

The Gill/Murray Cholesky factorization of a singular matrix C, adds 
a diagonal matrix E such that the standard Cholesky procedure is defined.  
Unfortunately it often increments C by an amount much larger than necessary providing 
a pseudo-Cholesky result that is further away from the intended result.  
Schnabel and Eskow (1990) improve on the C+E procedure of Gill and Murray by
applying the Gerschgorin Circle Theorem to reduce the infinity norm of the E matrix. The strategy is to calculate
delta values that reduce the \emph{overall} difference between the singular matrix and
the incremented matrix.   This improves the Gill/Murray approach of incrementing diagonal
values of a singular matrix sufficiently that Cholesky steps can be performed.

\vbox{
This technique is complex to describe but simple to use:

\begin{Schunk}
\begin{Sinput}
> S <- matrix(c(2, 0, 2.5, 0, 2, 0, 2.5, 0, 3), ncol = 3)
> sechol(S)
\end{Sinput}
\begin{Soutput}
      [,1]  [,2]     [,3]
[1,] 1.414 0.000 1.767767
[2,] 0.000 1.414 0.000000
[3,] 0.000 0.000 0.004262
attr(,"delta")
[1] 1.817e-05
\end{Soutput}
\begin{Sinput}
> t(T)
\end{Sinput}
\begin{Soutput}
     [,1]
[1,] TRUE
\end{Soutput}
\end{Schunk}
}

\section{References}\vspace{-5pt}
\renewcommand{\baselinestretch}{1}
\bibitem    % LEAVE THIS IN FOR FORMATTING PURPOSES


\bibitem        Altman M, Gill J, McDonald MP (2003).
                \emph{Numerical Issues in Statistical Computing for the Social Scientist.}
               John Wiley \& Sons, New York.

\bibitem        Belsley DA (1991). \emph{Conditioning diagnostics, collinearity
                 and weak data in regression.}
               John Wiley \& Sons, New York.

\bibitem			Chaitin-Chatelin F, Traviesas-Caasan E (2004b).  
            ``Qualitative Computing.'',
               In Bo Einarsson (ed.), \emph{Accuracy and Reliability in Scientific Computing.}
         SIAM Press, Philadelphia.

\bibitem        Cheng C, Van Ness JW (1999).
                \emph{Statistical Regression with Measurement Error.}
                 Arnold, London.

\bibitem        de Haan, L (1981).
        ``Estimation of the Minimum of a Function Using Order Statistics.''
        \emph{Journal of the American Statistical Association}, {\bf 76}, 467-9.

\bibitem        Fuller WA (1987).
                \emph{Measurement Error Models.}
                John Wiley \& Sons, New York.

\bibitem        Gill J \& King G (2004).  
              ``What to do When Your Hessian is Not Invertible: 
                Alternatives to Model Respecification in Nonlinear Estimation.'' 
                \emph{Sociological Methods and Research}, \textbf{32}(1), 54-87. 

\bibitem Hendrickx J, Belzer B, te Grotenhuis M,  Lammers J (2004).
       ``Collinearity Involving Ordered and Unordered Categorical Variables.''
         Presented at ``RC33 conference in Amsterdam, August 17-20''.
	 URL \url{http://www.xs4all.nl/~jhckx/perturb/}.

\bibitem 	Imai K, King G,  Lau O (2005). ``Zelig:
  		Everyone's Statistical Software.'' R package version 2.4-5.
  		\url{http://gking.harvard.edu/zelig}


\bibitem        Longley, JW (1967).
                ``An Appraisal of Computer Programs for the Electronic Computer from the Point o
f View of the User.''
                \emph{Journal of the American Statistical Association}, {\bf 62}, 819-41.


\bibitem        Schnabel RB, Eskow E (1990).
                ``A New Modified Cholesky Factorization.''
                \emph{SIAM Journal of Scientific Statistical Computing}, {\bf 11}, 1136-58.

\bibitem         Veall MR (1990).
                 ``Testing for a Global Maximum in an Econometric Context.''
                  \emph{Econometrica}, {\bf 58}, 1459-65.
                
     
\bibitem      Venables WN, Ripley BD (2002). \emph{Modern Applied
              Statistics with S. Fourth Edition.} Springer, New York.


\newpage
\end{document}
%\VignetteIndexEntry{accuracy}
