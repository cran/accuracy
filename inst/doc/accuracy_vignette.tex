%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% LATEX HEADERS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[11pt]{article}
\usepackage{rotating}
\usepackage[dvips]{epsfig}
\usepackage{multirow}
\usepackage{amsfonts,amssymb,amsmath}
\usepackage{array}
\usepackage{url}
\usepackage{thumbpdf}

\usepackage{vmargin}
    \setpapersize{USletter}
    \setmarginsrb{1.0in}{1.0in}{1.0in}{1.0in}{0.5in}{0.2in}{0in}{0.2in}
\renewcommand{\baselinestretch}{1.25}
\renewcommand{\bibitem}{\vskip 2pt\par\hangindent\parindent\hskip-\parindent}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% SHORTCUTS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newcommand{\y}{\mathbf{y}}
\newcommand{\Y}{\mathbf{Y}}
\newcommand{\x}{\mathbf{x}}
\newcommand{\X}{\mathbf{X}}
\newcommand{\B}{\boldsymbol{\beta}}
\newcommand{\cov}{\text{Cov}}
\newcommand{\var}{\text{Var}}
\newcommand{\T}{\boldsymbol{\theta}}
\newcommand{\EP}{\boldsymbol{\epsilon}}

%%%%%%%
%%% Vignette Headers
%%%%%%%
%\VignetteIndexEntry{Tools For Accurate and Reliable Statistical Computing}
%\VignetteTitle{Accuracy Overview}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% DOCUMENT STARTS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{/usr/lib64/R/share/texmf/Sweave}
\begin{document}

%% Automatically supplied by Jstatsoft headers
%
%\begin{large}\begin{center}
%    \textsc{``R Modules for Accurate and Reliable Statistical Computing''}\\[22pt]
%\end{center}\end{large}
%\vspace{11pt}
%\begin{center}
%    Micah Altman, \texttt{Micah\_Altman@harvard.edu}\\
%    Jeff Gill, \texttt{jgill@ucdavis.edu}\\
%    Michael McDonald, \texttt{mmcdon@gmu.edu}\\
%\end{center}
%


\newpage

\section{Using data perturbations for sensitivity analysis}

An easy-to-use exploratory test for numerical and measurement error stability for a given model is 
to introduce small random perturbations to the data, on the order of the measurement error of the 
instruments used to collect it, and recalculate the estimate. When the estimates produced using 
this technique vary greatly, the model estimation is necessarily unstable. And although the 
converse is not necessarily true, where a model is already known to be statistically appropriate,
this type of sensitivity analysis will give the researcher greater confidence that the their 
results are robust to numerical and measurement error.

We have developed a package in R that makes perturbation-based sensitivity
 analysis simple to apply and to interpret. For most models this running
 a sensitivity analysis involves only two steps.                             
\begin{enumerate}
\item Specify the data, model, and model options for the unperturbed model, and optionally,
the error functions for the perturbation.  
\item Use \texttt{summary()} or \texttt{plot(summary())} to see the sensitivity of the parameter estimates
to perturbations.
\end{enumerate}
Perturb works automatically almost with any \textbf{R} model, such as \texttt{lm}, 
\texttt{glm}, and \texttt{nls}, that accepts \texttt{data} 
as an argument to supply data and  that returns estimated coefficients through
\texttt{coef()}.

The example below shows how to conduct a sensitivity analysis of the classic 
analysis by Longley (1964) using \texttt{sensitivity()} and default noise
functions. 

\vbox{
\begin{Schunk}
\begin{Sinput}
> plongley = sensitivity(longley, lm, Employed ~ .)
> print(summary(plongley), digits = 4)
\end{Sinput}
\begin{Soutput}
[1] "Sensitivity of coefficients over  50  perturbations:"
             Perturb Est. (Orig. Est.) (Orig. Stderr)       2.5%      97.5%
(Intercept)    -3.140e+03   -3.482e+03      8.904e+02 -4.606e+03 -1.745e+03
GNP.deflator    1.527e-02    1.506e-02      8.491e-02 -8.002e-02  1.372e-01
GNP            -2.852e-02   -3.582e-02      3.349e-02 -6.490e-02  2.445e-02
Unemployed     -1.907e-02   -2.020e-02      4.884e-03 -2.478e-02 -1.153e-02
Armed.Forces   -9.971e-03   -1.033e-02      2.143e-03 -1.263e-02 -7.096e-03
Population     -4.582e-02   -5.110e-02      2.261e-01 -3.682e-01  2.832e-01
Year            1.652e+00    1.829e+00      4.555e-01  9.407e-01  2.401e+00
             [Unstable]
(Intercept)            
GNP.deflator           
GNP                    
Unemployed             
Armed.Forces           
Population             
Year                   
\end{Soutput}
\end{Schunk}
}
\vbox{
The sensitivity results can also be expressed in plot format:
\begin{Schunk}
\begin{Sinput}
> plot(summary(plongley))
\end{Sinput}
\end{Schunk}
\includegraphics{accuracy_vignette-003}
}

This is a rare example of a model that is very sensitive to noise.
Even so, note that the small amounts of  noise applied 
tremendously alter some of the estimated coefficients, but not others.  In most 
practical cases, however,  the substantive implications of your model will remain the same
across the sensitivity analysis -- in which case, you can publish them with
greater confidence. 

If error functions are not specified, a default  set of error function will be 
selected based on measurement types of the variable: continuous, ordered, or unordered.
Continuous variables, by default are subject to a small amount of
 mean-zero component-wise uniformly distributed
noise, which is typical of instrumentation-driven measurement error. Ordered factors
are assigned a small probability of having observations reclassified to the neighboring classification,
and unordered factors have a small probability of being reassigned to another legal value.

Alternatively, one can specify the error functions to use yourself, or use one of many
supplied by \emph{accuracy}.  The \emph{accuracy} package comes with a wide range 
of noise functions for continuous distributions,  and random reclassification of factors. 
\footnote{The \texttt{perturb} package for collinearity diagnosis by Hendrickx, et. al (2004) (which was
developed for \texttt{R} after the \texttt{accuracy} package) provides additional
methods for randomly reclassifying factors that via its \texttt{reclassify()} function. 
This function can be used in conjunction with \texttt{accuracy}. 
Hendrickx, et. al also provide a number of collinearity 
diagnostics, including one based on data perturbations. } 

Your choice of error functions should be chosen to reflect measurement error model for the specific
data you are using. In numerical analysis,  uniform noise  is often used since this is what would be
expected from  simple rounding error.  Normal random noise is commonly used in statistics,
under the assumption that measurement error is the sum of multiple independent error processes. In 
addition, when normal perturbations are used, the result can be interpreted, for many models, as equivalent
to the results of running a slightly perturbed \emph{model} on unperturbed data.  In some cases,
like discrete or ratio variables, other forms of noise are necessary to preserve
the structure of the problem. (see for example, Altman, Gill, McDonald 2005). 
The magnitude of the noise is also under the control of the researcher. Most
use a magnitude equivalent to the researchers estimate of the underlying measurement error in the data.
Noise is usually adjusted to the size of each component, since this better
preserves the structure of the problem, however in some cases the underlying
measurement error model may imply norm-wise scaling of the noise. For more
information on noise distributions and measurement error models see , e.g., 
Belsley 1991, Chaitin-Chatelin \& Traviesas-Caasan  (2004b), Caroll et. al (1995), Cheng \& Van Ness (1999), Fuller (1987).

If multiple plausible measurement error models can be hypothesized,
we recommend that \texttt{sensitivity} be run multiple times with different noise specifications,
However, in our  experience with  social science analyses, the choice
of error model does not tend to  effect, in practice, the substantive conclusions from the 
sensitivity analysis.

Some researchers omit perturbations to outcome variables, since, in terms of
statistical theory, mean-zero measurement error on outcome variables (as opposed to explanatory variables)
contribute only to increased variance in estimates, not bias. While this attitude is well-justified 
in the context of statistical theory, it is not similarly justified in the computational realm. If the estimation
of a model is computationally unstable, errors in the outcome variable may have large and unpredictable
 biases on the model estimate. Hence, the conservative default in our package is to subject all variables to perturbation, although options are available to completely control the form and magnitude of
all perturbations.

\vbox{
Consider this example, which shows a sensitivity analysis of the anorexia analysis
described in Venables and Ripley (2002). In this case, we leave the dependent
variable unperturbed, by assigning it the \emph{identity} error function.

\begin{Schunk}
\begin{Sinput}
> data(anorexia, package = "MASS")
> panorexia = sensitivity(anorexia, glm, Postwt ~ Prewt + Treat + 
+     offset(Prewt), family = gaussian, ptb.R = 100, ptb.ran.gen = c(PTBi, 
+     PTBus, PTBus), ptb.s = c(1, 0.005, 0.005))
> print(summary(panorexia), digits = 4)
\end{Sinput}
\begin{Soutput}
[1] "Sensitivity of coefficients over  100  perturbations:"
            Perturb Est. (Orig. Est.) (Orig. Stderr)    2.5%   97.5% [Unstable]
(Intercept)      49.7136      49.7711        13.3910 48.9888 50.6912           
Prewt            -0.5648      -0.5655         0.1612 -0.5767 -0.5561           
TreatCont        -4.0940      -4.0971         1.8935 -4.1725 -4.0291           
TreatFT           4.5636       4.5631         2.1333  4.4790  4.6572           
\end{Soutput}
\end{Schunk}
}

Finally, if a model in \texttt{R}  does not take a \texttt{data} argument or does not
return coefficients through the  \texttt{coef} method, it is usually 
only a matter of a few minutes to write a
small wrapper that calls the original model with appropriate data, and that
provides a  \texttt{coef} method for retrieving the results. (Alternatively, you
might to choose to run such models in \texttt{Zelig}, as described in the next section.)

For example, the \texttt{mle} function for maximum-likelihood estimation does
not have an explicit \texttt{data} option. Instead, it normally receives data implicitly
through the log-likelihood function, \texttt{ll}, passed into it. To adapt it for use
in \texttt{sensitivity} we simply construct a another function that accepts data and a 
log-likelihood function separately, constructs a temporary log-likelihood
function with the data passed in the environment, and then calls \texttt{mle}
with the temporary function:
\vbox{
\begin{Schunk}
\begin{Sinput}
> mleD <- function(data, lld, ...) {
+     f = formals(lld)
+     f[1] = NULL
+     ll <- function() {
+         cl = as.list(match.call())
+         cl[1] = NULL
+         cl$data = as.name("data")
+         do.call(lld, cl)
+     }
+     formals(ll) = f
+     mle(ll, ...)
+ }
\end{Sinput}
\end{Schunk}
}

Finally, construct the log-likelihood function to accept data. As in this example, which
is based on the documented example in the \texttt{Stats4} package:

\vbox{
\begin{Schunk}
\begin{Sinput}
> library(stats4)
> dat = as.data.frame(cbind(0:10, c(26, 17, 13, 12, 20, 5, 9, 8, 
+     5, 4, 8)))
> llD <- function(data, ymax = 15, xhalf = 6) -sum(stats::dpois(data[[2]], 
+     lambda = ymax/(1 + data[[1]]/xhalf), log = TRUE))
> print(summary(sensitivity(dat, mleD, llD)), digits = 4)
\end{Sinput}
\begin{Soutput}
[1] "Sensitivity of coefficients over  50  perturbations:"
      Perturb Est. (Orig. Est.)    min   2.5%  97.5%    max
ymax         24.92       24.993 21.862 24.679 25.069 25.181
xhalf         3.09        3.057  2.981  2.999  3.344  3.879
\end{Soutput}
\end{Schunk}
}

\subsection[Sensitivity analysis using Zelig]{Sensitivity analysis using \texttt{Zelig}}

\texttt{Zelig} (Imai, et. al 2005) is an easy-to-use R package that can estimate and help interpret
 the results of a large range of statistical models. \texttt{Zelig} provides
 a uniform interface to these models the \texttt{Accuracy} package
utilizes to enable sensitivity analyses. In addition, \texttt{Accuracy} can
also be used to perform sensitivity analyses of the robust alternatives, simulated 
predicted values, expected values, first differences, and risk ratios that \texttt{Zelig}
produces for all the models it supports.  \footnote{ \texttt{Zelig} also
 integrates nonparametric matching methods as an optional preprocessing step. Thus
 \texttt{Accuracy} supports sensitivity analysis of models subject to such pre-processing as well.}
So, using these packages together is an easy way to analyze the sensitivity 
of \emph{predicted values} to measurememnt error.


To illustrate, we replicate Longley's analysis (above), using
\texttt{zelig()} (instead of \texttt{lm()}) to run the OLS model, and the convenience function
 \texttt{sensitivityZelig()} to run the sensitivity analysis:
\vbox{
\begin{Schunk}
\begin{Sinput}
> if (require("Zelig", quietly = T, warn.conflicts = F)) {
+     zelig.out = zelig(Employed ~ GNP.deflator + GNP + Unemployed + 
+         Armed.Forces + Population + Year, "ls", longley)
+     perturb.zelig.out = sensitivityZelig(zelig.out)
+ }
\end{Sinput}
\begin{Soutput}
## 
##  Zelig (Version 2.8-3, built: 2007-05-29)
##  Please refer to http://gking.harvard.edu/zelig for full documentation 
##  or help.zelig() for help with commands and models supported by Zelig.
##
\end{Soutput}
\end{Schunk}
}

Just as above, \texttt{summary()} and \texttt{plot(summary())} can be used summarize
the sensitivity of the model coefficients.  In addition, we can use the 
\texttt{Zelig} methods \texttt{setx} and \texttt{sim} to simulate various
quantities of interest. And when \texttt{summary()} and
 \texttt{plot()} are used, they will display a \emph{sensitivity analysis} 
of the predicted values.

For example, this code generates predictions of the distribution of the explanatory variable,
`Employed', around the  point where `Year' equals 1955, 
and the other variables are at their means, and creates a profile plot of 
the predicted distribution of the explanatory variable:

\vbox{
\begin{Schunk}
\begin{Sinput}
> if (require("Zelig", quietly = T, warn.conflicts = F)) {
+     setx.out = setx(perturb.zelig.out, Year = 1955)
+     sim.perturb.zelig.out = psim(perturb.zelig.out, setx.out)
+     print(summary(sim.perturb.zelig.out))
+ }
\end{Sinput}
\begin{Soutput}
**** 50  COMBINED perturbation simulations 

  Model: ls 
  Number of simulations: 1000 

Values of X 
     (Intercept) GNP.deflator   GNP Unemployed Armed.Forces Population Year
1947           1        101.7 387.7      319.3        260.7      117.4 1954

Expected Values: E(Y|X)
      mean     sd 2.5% 97.5%
1947 65.31 0.1138 65.1 65.54
\end{Soutput}
\end{Schunk}
\begin{Schunk}
\begin{Sinput}
> if (require("Zelig", quietly = T, warn.conflicts = F)) {
+     plot(sim.perturb.zelig.out)
+ }
\end{Sinput}
\begin{Soutput}
**** 50  COMBINED perturbation simulations 
\end{Soutput}
\end{Schunk}
\includegraphics{accuracy_vignette-009}
}


\subsection{True random numbers through entropy collection}

`Random' numbers aren't.  The numbers provided by routines such as \texttt{runif()} are not genuinely random.
 Instead, they are \emph{pseudo-random number generators}
(PRNGs), deterministic processes that create a sequence of numbers. 
Pseudo-random number generators start with a single ``seed'' value (specified by the user or left at defaults)
 and generate a repeating sequence with a certain
fixed length, or  period $p$. This sequence is statistically
similar, in limited respects, to random draws from a uniform
distribution.

The earliest PRNGs, still in use in some places, and used in early versions of R, is the Linear Congruential Generator (LCG), 
which is defined as:

\begin{align}\label{Congruential.Generator}
    LCG(a,m,s,c)&\equiv \nonumber\\
            x_{0} &=s,  \nonumber\\
            x_{n} &=(ax_{n-1}+c)\bmod{m}.
\end{align}
(All parameters are integers, and in practice $x$ is usually divided by $m$ to yield numbers between zero and one.) 

This function  generates a sequence of numbers between $[0,m-1]$ which appears to be, using some tests, uniformly distributed in that range.  Other PNRG's are more complex, but share with the LCG the  fundamental properties of determinism and periodicity.  See (Gentle 1998) for an extensive treatment of modern PRNG's and theory.

\texttt{R} provides several high quality PRNG's natively, and packages such as  
\texttt{gsl}, \texttt{rstream} and \texttt{rpsrng} which  can be used to generate
quasi-random number streams, and concurrent  PRNG streams.
Regardless of the particular PRNG algorithm used, however, a PRNG cannot
perfectly mimic a random sequence. And, in fact, there is no complete
theory to describe the domains for which PRNG and true random sequences 
can be considered interchangeable.  In addition, the theory
on which PRNG's are based assumes that the seed itself is \emph{truly} random.

The \texttt{runifT()} routine is different from other random number generators in R. 
It delivers true random numbers based on
entropy collected from external physical sources of randomness.

Two sources of randomness are currently supported. On Unix and Linux system, the kernel gathers environmental noise from 
device drivers and other sources into a system entropy pool.
This pool can be accessed through the '/dev/random' pseudo-device.
Alternatively, the ``Hotbits'' web server, run by FourmiLab provides random bytes based on radioactive decay.

Using either source, these routines will retrieve random bits in chunks,
 and keep them in a local pool. This pool will be used as necessary to 
 satisfy calls to \texttt{runifT()} and \texttt{resetSeed()},
  and will be automatically refreshed from the external sources when empty. 
  If external sources are unavailable, the pool is refreshed using standard PRNG's.

Entropy collection is relatively slow compared to PRNGS. So, these routines
are most efficient for generating either small numbers of very-high-quality random
numbers (e.g. for cryptography) or for seeding (and regularly reseeding) PRNG's. 
The function \texttt{resetSeed()} sets the seed for the standard PRNG\'s using
true random bits.  The \texttt{runifS()} automates this process further, by  reseeding 
runif() with random values, periodically to improve the random properties of the
 resulting sequence:

\vbox{
\begin{Schunk}
\begin{Sinput}
> birthday <- function(x, n = 2^20) {
+     spacings = diff(trunc((x * .Machine$integer.max)%%n))
+     tab = table(spacings)
+     tab = tab[which(tab > 1)]
+     chisq.test(sample(tab, 200, replace = T))
+ }
> resetSeed()
\end{Sinput}
\begin{Soutput}
  [1]         403         387   561691117  1182683816  -228431327   139247307
  [7] -1441917399 -1707762879 -1955944014  -383556462   547035668   577826214
 [13]  2016108073  1374362608  1491824980 -1072864251  1740962019 -1781252980
 [19]  1794404497 -2135066255  -553219516   263208233   -70744188 -1206328120
 [25]    12731936 -1850419744   629100474  1664837904  2122670069  1269417563
 [31]  -291620394  -420864054 -2072317912   217037128  -442666806  -643303835
 [37] -1716172192  1242403307   326126058  1779241683 -1016333421   275302979
 [43]  1551995480  2107679576 -1925135940  1438849678  1833903446  1976343412
 [49] -1075217985   259295280 -1032038758   497651073  2139369816  1648049842
 [55]  -294179317  -951813652   562474673  -354164676  1650477762  -953182797
 [61]   994044390  2094042234   614250660  1185597996  1274026849  1850986448
 [67]   879648368 -1200072576  1068124398  -888584980  -549480282  1092742983
 [73]  -319427622 -2037856941 -1636469960  1192389701   843171305  -655664163
 [79]  -948981693  -820746991  1840676717  1375835522 -1407726257 -1825839711
 [85]  1769637896  1889639837  1721634974  1679707613  2039596184 -1491191910
 [91] -1694983039  1893203953 -2026300773  -938351942 -1262669690  -433749879
 [97]   361974963  -809196171  -719632643  1560723784  -754823164 -1910340581
[103] -1847531698   416568913  1803485642  -542059814  -767083062  -296106495
[109]  -301808560  2028581028  -395917545    92184889   654583171  -138315193
[115] -1361272421  -692362537 -1415980139 -1567885663  -253069793  1397615935
[121]  -113638148  1508707518  -747084941  1776363715  1392239612  -544164517
[127]  2061825108 -2030509890  1275311933 -2089813325   -59338614   272166791
[133]  1160573491   655809349 -1149009437 -1419992850 -1663285282  2123608363
[139]  1136698193 -1609739277 -1631855883   108792610 -1987360972  2061100904
[145] -2042078067  1930775374  1701421092  1902323111  -248556821  -426311045
[151]  1114284641   691118523  1160553484    73540694   521011483 -2092627414
[157]   -27901028 -1021668371  -388916632  1144570610   479815167  1247715553
[163]   321237864   117071666  -152480078   -27337008  1839926817   576621544
[169]  -591606788    83695522   145041164   565506371 -1093080529    91581673
[175]  1883670551  -769982727 -1575079445   524705123 -1525932233 -1501195065
[181]   645211140 -1112657447   -59983788  -822996542  2011087776   -65516351
[187]   817183949    71829725  -478345337   384028087  1105674864  1417598384
[193]   478126602 -2126897546  1262707324  1348296761  1331006746  1408602060
[199]   918015269   394476733   669176784   704847960  -804150628 -1176810852
[205]  1300702077  2057892960   174914837   487065225 -2114193216  -300121896
[211]  -396103441  1984936553 -1947493893  1897164641   946186184  -524137898
[217] -1512432393 -1521704021   359983356   190493288  1556907791 -1003237888
[223] -2044556529  -632070482   804537866   225933469   982437209  -905744479
[229] -1734584308  1973061816  2123816493   -19134921  -173500414  1290222841
[235]  1953020553  2071156999  -795982252 -1963456547  -637917013  1281872604
[241]  1514427923  -100873612   236011398 -2081042024  1529069599 -1409684510
[247]  -814103684  -192813803  1897166317  -334350374 -1360803350  1070723031
[253]  2144658182  -978818312   178603986 -1484684091  1600303978  -866069110
[259] -1783493188 -1317354907 -1901520527 -1304022840   688102497 -1953217216
[265]  -602894896  -872795526  -635924476  1493193027  -883116859  1870620445
[271]   813527502 -1759071877   -41199753  1593129383   252693723   114792935
[277]   590147897 -2005941481   142911028  1381679667   641260905  -493430930
[283]  1525026266   310624338   324270448 -1576961178 -1679140796  1650740253
[289]  -900951958 -1894588482  -722172095  1649375895  1354430928 -1109422677
[295]  2028926592   593293303   192519858   169242678    25413511  -687118669
[301]   590211220  1000557039 -1221871425 -1606939416 -1836617609  1566172965
[307]  -450961290  1169150209  1145691160 -1904834513  1262897824  2128043552
[313]  2017035326   706944975   884891577  -616085280 -1605562744 -1754055703
[319] -1072825115 -2131008022 -1519490631  1920612236 -1218734081   333295233
[325]  -705588350 -1557066465  -954754139  2074436407  1287432267  -818005708
[331]   796267447 -1566305297  -561667110  1840707628   158319039   306251912
[337]   -49193987  1576990097  1048726548  -940679774  1626058898   588235438
[343] -1173156665 -1482091413   -98507107 -1961044457 -1922690051   990766123
[349]  1643857515 -1177477935 -1596099648 -1607327278  1261997970  -399437185
[355] -1233788312  1364727126 -1024516986  1188662175  -884942480  1673308168
[361]  -274913675  1905295163  2026122535   959431807 -1240866348  -155773970
[367]  2089800274  -111389762 -1447780902  -449374327 -1635065368 -1620122615
[373]  -708418764 -1193163787 -1114741986 -1885195751  -390226912  1695840085
[379]   463640591 -1700557941  -544330080   941285296 -1598141643 -2085689712
[385]   745891639 -1930349939  -466403198   290337797 -1184883133  1383672124
[391]    -8988162  1168476321 -1239444120   936550897   633343420 -1672796713
[397]   393113085 -1727500456  1235845570  -933696413  -351238998   -29573816
[403]   933415657   682942055   652121351  1517269617   412071304  1887698436
[409]  -612594872 -2086201891  1519323804 -1929692278 -1848196042   303976021
[415] -1327973238  1618106448  -562741113  1227409978  -730248327  1029999724
[421] -1721130129  -752117917  -839084810  1774136664     1461336  -144905962
[427]   -46366562  1237847121  -816696033   245682424 -1621545182   665264412
[433]  -490831143  2044084864  1572520123  1163770695  -434402249   236661223
[439]  2002898711   537264605  -350518519  -132324892   455005688  -527866378
[445] -1332679392 -1845089280 -1621175592  -612634146  1862797407   100797833
[451]   905781786  -850826402  1561022811 -1788385007  -541553543 -1271934508
[457]   386586335 -2034919555  1064187767   195695709 -1126466789  1833235356
[463]  2044709832   868689178 -1709026969  -471484980  -281367225   565512961
[469]  1638833295  -555129111  -562837492   699155308  1620138498  1371503891
[475]    40253259  -254617023 -1057715563    74040300     5543119   775066751
[481]  -806608692  -514424841  1507455951     6986504  -302890400  1534818235
[487]  -658096397  -609216312  1113156799    -5560555 -1664530638   261070854
[493] -1933224054  -399218148 -1002938258  -918644488   293047839 -1188632094
[499] -1118071734  2119917069   778700093  1271782197  1414351735  -200738514
[505]  1422913233 -1084283266  2000724904  -907099497  -186960667   829047813
[511]    49063210  -266954418  1532164083   726387664 -1463073974  1907297728
[517]   556050724 -1171209326  1387003253 -1629264070   805090789   -46289495
[523]  2141207641    87133896     4231797  -124394537   741018903   392242318
[529] -1532771910  -357362038   258595920   -92596867  -681086151   660617083
[535]   788965047  1693366801   407215422   777873749   909906747   369698581
[541]    15834029  -785465611 -1088007088  1880285334   900408127  -926419700
[547]  1919220533 -2081346485  1519869616  -140029951   455646876   833199958
[553] -1589865693  -122524486  1431439661 -1631597465  -815736554   -76357181
[559]   476562891  -426260094 -1867983969   455441795  1642222257  -219916748
[565]   678506596  -224063262   503672859   906684614  -842727205    39446492
[571] -1934389094  1995880952  -782308668   111726006   982960333  1029886625
[577]  -536064107  2051207077  1329808412 -1105327323 -1475510038  -696428208
[583] -1489642668 -1494344675   682214233  -809033593  -833326447   164395538
[589] -1307142538   -17018732  -634314063    27167086 -2046301740  -779458444
[595]  1892933522   412272163 -1237785949 -1228197494   420730789   150167419
[601]  1121843435   831804694  -232334700   313820359  1410991548  -431105969
[607] -1474307332  -515778304  -182567700 -1607663563 -1796252290   709376634
[613]   580123455  -177491193  1648785663  1368982862   -86768174 -1043551391
[619]  1788183946  1314779894  1925209454   435725992 -1369226448  1510060924
[625]    -4390366 -2145620501
\end{Soutput}
\begin{Sinput}
> y = runif(1e+06)
> birthday(y)
\end{Sinput}
\begin{Soutput}
	Chi-squared test for given probabilities

data:  sample(tab, 200, replace = T) 
X-squared = 31.69, df = 199, p-value = 1
\end{Soutput}
\begin{Sinput}
> y = runifS(1e+06)
> birthday(y)
\end{Sinput}
\begin{Soutput}
	Chi-squared test for given probabilities

data:  sample(tab, 200, replace = T) 
X-squared = 32.07, df = 199, p-value = 1
\end{Soutput}
\end{Schunk}
}
\subsection{Tests for global optimality}

The estimation of many statistical models rests on finding the global optimum
to a user-specified non-linear function. R provides a number of tools for such
estimations, including \texttt{nlm()}, \texttt{nls()}, \texttt{mle()}, 
\texttt{optim()}  and \texttt{constrOptim()}. 

All of these functions rely on local search algorithms, and the results they
return may depend on the starting point of the search. Maximum likelihood functions, non-linear-regression models, and the like, are not guaranteed to 
be globally convex in general. And even where convexity is guaranteed by statistical theory, inaccuracies in statistical computation can sometimes induce false local optima (discontinuities that may cause local search algorithms to converge, or at least stop).  A poor or unlucky choice of starting values may cause a search algorithm to converge at a local optimum, which may be far from the real global optimum of the function.  Inferences based on the values of the parameter at the local optimum will be incorrect. 

Knowing when a function has reached its true maximum is
something of an art. While the plausibility of the solution
in substantive terms is often used as a check, relying solely on the expected
answer as a diagnostic might bias researchers toward Type I errors. Diagnostic
tests are therefore useful to provide evidence that computed
solution is the true solution.

A number of strategies related to the choice of   starting values
 have been formalized as tests or global optimality. In this package we 
 implement two. The `Starr' test and the `Dehaan' test.
\footnote{In addition to these tests, the \texttt{R} user may also wish to investigate
the \texttt{bhat} package, which can generate diagnostic profile likelihood plots.} 
\footnote{If this indicats that the optimum has not been reached, the user may consider
using  heuristics designed for non-smooth optimization problems, such as the simulated annealing option for \texttt{optim()}, or the optimizers provided by  the \texttt{gafit}, \texttt{genalg}, 
\texttt{rgenoud} modules.}
 
The intuition behind the Starr test statistic is to run the optimization from 
different starting points to observe 'basins of attraction', and then to
estimate the number of \emph{unobserved} basins of attraction from the
number of observed basins of attraction.  The greater the number
of observed basins of attraction, the lower the probability that a
global optimum has been located.  This idea has been attributed to Turing (1948),
and the test statistics was developed  by Starr (1979):
\begin{equation}\label{Starr.test.equation}
    V_{2}=\frac{S}{r}+\frac{2D}{r\left( r-1\right)}.
\end{equation}\index{optimization algorithm!global optimality
tests!Starr test} Here $V_2$ is the probability a convergence
point has not been observed, and $r$ is the number of randomly
chosen starting points. $S$ is the number of convergence points
that were produced from one (or a {\underline{S}}ingle) starting
value and $D$ is the number of convergence points that were
produced from two (or {\underline{D}}ouble) different starting
values. 

Finch, Mendell, and Thode (1989) demonstrate the value of the
statistic by analyzing a one parameter equation on a $[0,1]$
interval for $r = 100$. While the proposed statistic given by the
above equation is compelling, their example is similar to an
exhaustive grid search on the $[0,1]$ interval. 
(Starr's result is further generalizable for triples and
higher order observed clumping of starting values into their
basins of attraction, but Finch, Mendell, and Thode assert that
counting the number of singles and doubles is usually sufficient.)

The statistic may be infeasible to compute for an unbounded parameter space with 
high dimensionality. However, the intuition behind the statistic
can still  be soundly applied in these cases. If multiple local optima are identified over the
course of a search for good starting values, a researcher should
not simply stop once an apparent best fit has been found,
especially if there are a number of local optima which have basins
of attraction that were identified only once or twice. Our implementation
of the Staff test provides a ready-to-use-interface that can be 
easily incorporated into a search of the parameter space for good  optimization 
starting values.

For computationally intensive problems, another test, by Veall (1990), drawing upon a 
result presented by de Haan (1981), may be more practical. The de Haan/Veall test relies on
 sampling the optimization function itself rather than
identifying basins of attraction. A confidence interval for 
the value of the likelihood function's global optimum is generated from
the points sampled from the likelihood surface. This procedure is much faster than the Starr
test because  the likelihood function  is calculated  only once for each 
trial. As with starting value searches, researchers are
advised to increase the bounds of the search area and the number
of trials if the function to be evaluated has a high degree of
dimensionality or a high number of local optimum have been
identified.

Veall suggests that by using a random search and applying extreme asymptotic theory, a confidence interval for the candidate
solution can be formulated. The method, according to Veall (1990:
1460) is to randomly choose a large number, $n$, of values for the
parameter vector using a uniform density over the entire parameter
space. Call the largest value of the evaluated likelihood function
$L_1$ and the second largest value $L_2$. The $1-p$ confidence
interval for the candidate solution, $L^{'}$, is $[L_1,L^p]$
where:

\begin{equation}\label{Veall.test.equation}
    L^p =L_1+\frac{ L_1-L_2 }{ p^{-1/\alpha}-1 }
\end{equation}
and $\alpha = k/2$, where $k$ is some function that depends on $n$ such that
$k(n)\rightarrow 0$, as $k(n),n \rightarrow \infty$ (a likely candidate is
$k=\sqrt{n}$).

As Veall (1990: 1461) notes, the bounds on the search of the
parameter space must be large enough to capture the global maximum
and $n$ must be large enough to apply asymptotic theory. In Monte
Carlo simulations, Veall suggests that 500 trials are sufficient
for rejecting that a local optimum is not the \emph{a priori}
identified global optimum. 

\vbox{
Examples of applying both the dehaan and starr tests are below:
\begin{Schunk}
\begin{Sinput}
> data("BOD")
> stval = expand.grid(A = seq(10, 100, 10), lrc = seq(0.5, 0.8, 
+     0.1))
> llfun <- function(A, lrc) -sum((BOD$demand - A * (1 - exp(-exp(lrc) * 
+     BOD$Time)))^2)
> lls = NULL
> for (i in 1:nrow(stval)) {
+     lls = rbind(lls, llfun(stval[i, 1], stval[i, 2]))
+ }
> fm1 <- nls(demand ~ A * (1 - exp(-exp(lrc) * Time)), data = BOD, 
+     start = c(A = 20, lrc = log(0.35)))
> ss = -sum(resid(fm1)^2)
> dehaan(lls, ss)
\end{Sinput}
\begin{Soutput}
[1] TRUE
\end{Soutput}
\end{Schunk}
}

\vbox{
\begin{Schunk}
\begin{Sinput}
> llb = NULL
> for (i in 1:nrow(stval)) {
+     llb = rbind(llb, coef(nls(demand ~ A * (1 - exp(-exp(lrc) * 
+         Time)), data = BOD, start = c(A = stval[i, 1], lrc = stval[i, 
+         2]))))
+ }
> starr(llb)
\end{Sinput}
\begin{Soutput}
[1] 0
\end{Soutput}
\end{Schunk}
}
       
\subsection{A generalized Cholesky method}

The generalized inverse is a commonly used technique
in statistical analysis, but the generalized Cholesky has not before
been used for statistical purposes, to our knowledge.
When the inverse of the negative Hessian does not exist, we suggest
two separate procedures to choose from.  One is to create a
\emph{pseudo-variance matrix} and use it, in place of the inverse, in
an importance resampling scheme.  In brief, applying a generalized
inverse (when necessary, to avoid singularity) and generalized
Cholesky decomposition (when necessary, to guarantee positive
definiteness) together often produce a pseudo-variance matrix for the
mode that is a reasonable summary of the curvature of the posterior
distribution. This method is developed and analyzed in detail in (Gill and King, 2004),
here we provide a brief sketch.

The Gill/Murray Cholesky factorization of a singular matrix C, adds 
a diagonal matrix E such that the standard Cholesky procedure is defined.  
Unfortunately it often increments C by an amount much larger than necessary providing 
a pseudo-Cholesky result that is further away from the intended result.  
Schnabel and Eskow (1990) improve on the C+E procedure of Gill and Murray by
applying the Gerschgorin Circle Theorem to reduce the infinity norm of the E matrix. The strategy is to calculate
delta values that reduce the \emph{overall} difference between the singular matrix and
the incremented matrix.   This improves the Gill/Murray approach of incrementing diagonal
values of a singular matrix sufficiently that Cholesky steps can be performed.

\vbox{
This technique is complex to describe but simple to use:

\begin{Schunk}
\begin{Sinput}
> S <- matrix(c(2, 0, 2.5, 0, 2, 0, 2.5, 0, 3), ncol = 3)
> sechol(S)
\end{Sinput}
\begin{Soutput}
      [,1]  [,2]     [,3]
[1,] 1.414 0.000 1.767767
[2,] 0.000 1.414 0.000000
[3,] 0.000 0.000 0.004262
attr(,"delta")
[1] 1.817e-05
\end{Soutput}
\begin{Sinput}
> t(T)
\end{Sinput}
\begin{Soutput}
     [,1]
[1,] TRUE
\end{Soutput}
\end{Schunk}
}

\section{References}\vspace{-5pt}
\renewcommand{\baselinestretch}{1}
\bibitem    % LEAVE THIS IN FOR FORMATTING PURPOSES


\bibitem        Altman M, Gill J, McDonald MP (2003).
                \emph{Numerical Issues in Statistical Computing for the Social Scientist.}
               John Wiley \& Sons, New York.

\bibitem        Belsley DA (1991). \emph{Conditioning diagnostics, collinearity
                 and weak data in regression.}
               John Wiley \& Sons, New York.

\bibitem			Chaitin-Chatelin F, Traviesas-Caasan E (2004b).  
            ``Qualitative Computing.'',
               In Bo Einarsson (ed.), \emph{Accuracy and Reliability in Scientific Computing.}
         SIAM Press, Philadelphia.

\bibitem        Cheng C, Van Ness JW (1999).
                \emph{Statistical Regression with Measurement Error.}
                 Arnold, London.

\bibitem        de Haan, L (1981).
        ``Estimation of the Minimum of a Function Using Order Statistics.''
        \emph{Journal of the American Statistical Association}, {\bf 76}, 467-9.

\bibitem        Fuller WA (1987).
                \emph{Measurement Error Models.}
                John Wiley \& Sons, New York.

\bibitem        Gill J \& King G (2004).  
              ``What to do When Your Hessian is Not Invertible: 
                Alternatives to Model Respecification in Nonlinear Estimation.'' 
                \emph{Sociological Methods and Research}, \textbf{32}(1), 54-87. 

\bibitem Hendrickx J, Belzer B, te Grotenhuis M,  Lammers J (2004).
       ``Collinearity Involving Ordered and Unordered Categorical Variables.''
         Presented at ``RC33 conference in Amsterdam, August 17-20''.
	 URL \url{http://www.xs4all.nl/~jhckx/perturb/}.

\bibitem 	Imai K, King G,  Lau O (2005). ``Zelig:
  		Everyone's Statistical Software.'' R package version 2.4-5.
  		\url{http://gking.harvard.edu/zelig}


\bibitem        Longley, JW (1967).
                ``An Appraisal of Computer Programs for the Electronic Computer from the Point o
f View of the User.''
                \emph{Journal of the American Statistical Association}, {\bf 62}, 819-41.


\bibitem        Schnabel RB, Eskow E (1990).
                ``A New Modified Cholesky Factorization.''
                \emph{SIAM Journal of Scientific Statistical Computing}, {\bf 11}, 1136-58.

\bibitem         Veall MR (1990).
                 ``Testing for a Global Maximum in an Econometric Context.''
                  \emph{Econometrica}, {\bf 58}, 1459-65.
                
     
\bibitem      Venables WN, Ripley BD (2002). \emph{Modern Applied
              Statistics with S. Fourth Edition.} Springer, New York.


\newpage
\end{document}
%\VignetteIndexEntry{accuracy}
